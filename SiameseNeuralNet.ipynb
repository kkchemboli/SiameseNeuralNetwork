{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f26d34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing std dependencies\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2710cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pytorch dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "from torchmetrics.classification import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066d1ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a65714d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'os.makedirs(POS_PATH)\\nos.makedirs(NEG_PATH)\\nos.makedirs(ANC_PATH)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating folders\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')\n",
    "'''os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06941f55",
   "metadata": {},
   "source": [
    "moving lfw images to negative folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b1ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in os.listdir('lfw_funneled'):\n",
    "    if os.path.isdir(os.path.join('lfw_funneled',directory,file)):\n",
    "            for file in os.listdir('lfw_funneled'+'/'+directory):\n",
    "        \n",
    "                \n",
    "                EX_PATH = os.path.join('lfw_funneled',directory,file)\n",
    "                NEW_PATH = os.path.join(NEG_PATH,file)\n",
    "                os.replace(EX_PATH,NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5114968",
   "metadata": {},
   "source": [
    "collecting anchor and positive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "795cbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    # cutting frame to 250 by 250px\n",
    "    frame = frame[0:250, 0:250 ,:]\n",
    "\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    #collecting anchors \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        pass\n",
    "\n",
    "    #collecting positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "      pass\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d30703",
   "metadata": {},
   "source": [
    "Preprocessing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae03fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import decode_image,read_file\n",
    "\n",
    "\n",
    "def preprocess(img_path):\n",
    "    img_bytes = read_file(img_path)\n",
    "    image = decode_image(img_bytes)\n",
    "    image = torchvision.transforms.Resize((100,100))(image)\n",
    "    image = image / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec5a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img,validation_img,label):\n",
    "    input_img = preprocess(input_img)\n",
    "    validation_img = preprocess(validation_img)\n",
    "    return (input_img,validation_img,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a568c73",
   "metadata": {},
   "source": [
    "Creating labelled dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c84b28a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6824, 0.6824, 0.6824,  ..., 0.4235, 0.4235, 0.4157],\n",
       "          [0.6824, 0.6824, 0.6824,  ..., 0.4275, 0.4275, 0.4275],\n",
       "          [0.6824, 0.6824, 0.6824,  ..., 0.4275, 0.4353, 0.4314],\n",
       "          ...,\n",
       "          [0.2000, 0.1961, 0.2157,  ..., 0.6039, 0.5961, 0.5843],\n",
       "          [0.1961, 0.2039, 0.2980,  ..., 0.6118, 0.6118, 0.6196],\n",
       "          [0.2000, 0.2353, 0.3412,  ..., 0.6157, 0.6196, 0.6078]],\n",
       " \n",
       "         [[0.5765, 0.5765, 0.5765,  ..., 0.3608, 0.3608, 0.3569],\n",
       "          [0.5765, 0.5765, 0.5765,  ..., 0.3608, 0.3647, 0.3686],\n",
       "          [0.5765, 0.5765, 0.5765,  ..., 0.3686, 0.3725, 0.3686],\n",
       "          ...,\n",
       "          [0.2157, 0.2118, 0.2314,  ..., 0.6824, 0.6784, 0.6667],\n",
       "          [0.2157, 0.2196, 0.3137,  ..., 0.6824, 0.6824, 0.6980],\n",
       "          [0.2235, 0.2549, 0.3569,  ..., 0.6824, 0.6863, 0.6784]],\n",
       " \n",
       "         [[0.5020, 0.5020, 0.5020,  ..., 0.3137, 0.3137, 0.2980],\n",
       "          [0.5020, 0.5020, 0.5020,  ..., 0.3176, 0.3176, 0.3059],\n",
       "          [0.5098, 0.5059, 0.5059,  ..., 0.3176, 0.3216, 0.3059],\n",
       "          ...,\n",
       "          [0.2627, 0.2549, 0.2745,  ..., 0.7451, 0.7373, 0.7216],\n",
       "          [0.2627, 0.2627, 0.3569,  ..., 0.7333, 0.7333, 0.7412],\n",
       "          [0.2667, 0.2980, 0.3961,  ..., 0.7333, 0.7333, 0.7255]]]),\n",
       " tensor([[[0.6784, 0.6784, 0.6745,  ..., 0.4118, 0.4235, 0.4157],\n",
       "          [0.6784, 0.6784, 0.6784,  ..., 0.4157, 0.4235, 0.4235],\n",
       "          [0.6824, 0.6824, 0.6824,  ..., 0.4196, 0.4275, 0.4235],\n",
       "          ...,\n",
       "          [0.2588, 0.2431, 0.2078,  ..., 0.4314, 0.6039, 0.6196],\n",
       "          [0.2706, 0.3020, 0.3373,  ..., 0.4314, 0.6000, 0.6118],\n",
       "          [0.4824, 0.4941, 0.4431,  ..., 0.4392, 0.6078, 0.6118]],\n",
       " \n",
       "         [[0.5725, 0.5725, 0.5686,  ..., 0.3529, 0.3608, 0.3490],\n",
       "          [0.5725, 0.5725, 0.5725,  ..., 0.3569, 0.3608, 0.3608],\n",
       "          [0.5765, 0.5765, 0.5765,  ..., 0.3608, 0.3647, 0.3608],\n",
       "          ...,\n",
       "          [0.1216, 0.1216, 0.1059,  ..., 0.3529, 0.5059, 0.5137],\n",
       "          [0.1647, 0.2157, 0.2784,  ..., 0.3569, 0.5059, 0.5098],\n",
       "          [0.4157, 0.4431, 0.4196,  ..., 0.3647, 0.5137, 0.5098]],\n",
       " \n",
       "         [[0.4980, 0.4980, 0.4941,  ..., 0.3098, 0.3059, 0.2902],\n",
       "          [0.4980, 0.4980, 0.4980,  ..., 0.3059, 0.3059, 0.2980],\n",
       "          [0.5020, 0.5020, 0.5020,  ..., 0.3059, 0.3059, 0.2980],\n",
       "          ...,\n",
       "          [0.1098, 0.1059, 0.0980,  ..., 0.2980, 0.4471, 0.4549],\n",
       "          [0.1529, 0.2039, 0.2667,  ..., 0.3020, 0.4471, 0.4510],\n",
       "          [0.3961, 0.4314, 0.4078,  ..., 0.3137, 0.4549, 0.4510]]]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "\n",
    "anchor_files = glob.glob(ANC_PATH + '/*.jpg')[:300]\n",
    "positive_files = glob.glob(POS_PATH + '/*.jpg')[:300]\n",
    "negative_files = glob.glob(NEG_PATH + '/*.jpg')[:300]\n",
    "\n",
    "\n",
    "# Dataset that returns (anchor_path, other_path, label)\n",
    "class PairedFilePathDataset(Dataset):\n",
    "    def __init__(self, anchor_files, other_files, label,transform = None):\n",
    "        assert len(anchor_files) == len(other_files), \"Anchor and other files must match in length\"\n",
    "        self.anchor_files = anchor_files\n",
    "        self.other_files = other_files\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.cache = {}  # in-memory caching\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchor_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        anchor_path = self.anchor_files[idx]\n",
    "        other_path = self.other_files[idx]\n",
    "        label_tensor = torch.tensor(self.label, dtype=torch.float32)\n",
    "         # Apply preprocessing function here\n",
    "        if self.transform:\n",
    "            anchor_path, other_path,label_tensor = self.transform(anchor_path, other_path,label_tensor)\n",
    "        sample = (anchor_path, other_path, label_tensor)\n",
    "        self.cache[idx] = sample  # cache it in memory\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Create positive and negative datasets\n",
    "positive_dataset = PairedFilePathDataset(anchor_files, positive_files, label=1,transform=preprocess_twin)\n",
    "negative_dataset = PairedFilePathDataset(anchor_files, negative_files, label=0,transform=preprocess_twin)\n",
    "\n",
    "# Combine datasets manually (no ConcatDataset nesting)\n",
    "full_dataset = positive_dataset + negative_dataset  # works because __getitem__ returns tuples\n",
    "\n",
    "\n",
    "# Custom collate function to return tuple directly\n",
    "def single_collate_fn(batch):\n",
    "    # batch is a list of 1 item if batch_size=1\n",
    "    anchor, other, label = batch[0]\n",
    "    return anchor, other, label\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "paired_dataloader = DataLoader(\n",
    "    full_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=single_collate_fn\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "example = next(iter(paired_dataloader))\n",
    "example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6629090",
   "metadata": {},
   "source": [
    "Building train and test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a15e181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(example[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a204cd6",
   "metadata": {},
   "source": [
    "Building a data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e72e238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "total_len = len(full_dataset)\n",
    "train_len = int(0.7 * total_len)\n",
    "test_len = total_len - train_len\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True      # shuffles the dataset every epoch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18419714",
   "metadata": {},
   "source": [
    "Building an embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b0fb455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(10, 10), stride=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 10)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 7)\n",
    "        self.conv3 = nn.Conv2d(128, 128, 4)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 5 * 5, 4096)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.sigmoid(self.fc1(x))  \n",
    "        return x\n",
    "\n",
    "model = Embedding()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e627db3",
   "metadata": {},
   "source": [
    "Creating siamese L1 distance layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3093fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(nn.Module):\n",
    "    def __init__(self)->None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,input_embedding,validation_embedding):\n",
    "        return torch.abs(input_embedding - validation_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9442a2",
   "metadata": {},
   "source": [
    "Creating siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fafa6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNeuralNetwork, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.L1Dist = L1Dist()\n",
    "        self.fc1 = nn.Linear(4096,1)\n",
    "\n",
    "    def forward(self,input_img,validation_img):\n",
    "        input_embedding = self.embedding(input_img)\n",
    "        validation_embedding = self.embedding(validation_img)\n",
    "\n",
    "        l1_distance = self.L1Dist(input_embedding,validation_embedding)\n",
    "        output = torch.sigmoid(self.fc1(l1_distance))\n",
    "        return output\n",
    "\n",
    "model = SiameseNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d9494",
   "metadata": {},
   "source": [
    "Creating Train step model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4506a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30846a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef524e2d",
   "metadata": {},
   "source": [
    "setting up checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6abffcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac28e1",
   "metadata": {},
   "source": [
    "setting up loss and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96ceac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c208f7fe",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d1b7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(batch):\n",
    "    model.train()\n",
    "    anchor_img,validation_img,label = batch\n",
    "    label = label.unsqueeze(1)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(anchor_img,validation_img)\n",
    "    loss = lossfn(output,label)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93b375",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b593dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,epochs):\n",
    "    for epoch in range(1,epochs+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, epochs))\n",
    "        for batch in data:\n",
    "            loss = train_one_epoch(batch)\n",
    "            print(f\"Epoch {epoch+1} Loss: {loss}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"{checkpoint_prefix}/siamese_epoch_{epoch}.pth\")\n",
    "            print(f\"Model saved at epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad27c40",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86a650a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/50\n",
      "tensor(0.6941, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 2 Loss: 0.6941414475440979\n",
      "tensor(0.6898, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 2 Loss: 0.6897565126419067\n",
      "tensor(0.6810, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 2 Loss: 0.6810295581817627\n",
      "tensor(0.6646, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 2 Loss: 0.6645702123641968\n",
      "tensor(0.6809, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 2 Loss: 0.6809360980987549\n",
      "\n",
      " Epoch 2/50\n",
      "tensor(0.6460, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 3 Loss: 0.646033763885498\n",
      "tensor(0.5600, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 3 Loss: 0.5599795579910278\n",
      "tensor(0.5813, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 3 Loss: 0.5812582969665527\n",
      "tensor(0.4297, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 3 Loss: 0.4297126531600952\n",
      "tensor(0.5201, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 3 Loss: 0.5201035141944885\n",
      "\n",
      " Epoch 3/50\n",
      "tensor(0.3565, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 4 Loss: 0.3565317988395691\n",
      "tensor(0.4575, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 4 Loss: 0.4575175940990448\n",
      "tensor(0.6075, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 4 Loss: 0.6075496077537537\n",
      "tensor(0.5528, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 4 Loss: 0.552769660949707\n",
      "tensor(0.4783, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 4 Loss: 0.4782605469226837\n",
      "\n",
      " Epoch 4/50\n",
      "tensor(0.4293, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 5 Loss: 0.4292607307434082\n",
      "tensor(0.5120, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 5 Loss: 0.5119721293449402\n",
      "tensor(0.5201, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 5 Loss: 0.5201240181922913\n",
      "tensor(0.4304, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 5 Loss: 0.4303756058216095\n",
      "tensor(0.3345, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 5 Loss: 0.3344675302505493\n",
      "\n",
      " Epoch 5/50\n",
      "tensor(0.4948, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 6 Loss: 0.4947727620601654\n",
      "tensor(0.3330, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 6 Loss: 0.33296501636505127\n",
      "tensor(0.4072, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 6 Loss: 0.4072052240371704\n",
      "tensor(0.4388, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 6 Loss: 0.4387923777103424\n",
      "tensor(0.2888, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 6 Loss: 0.28883570432662964\n",
      "\n",
      " Epoch 6/50\n",
      "tensor(0.3711, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 7 Loss: 0.37111154198646545\n",
      "tensor(0.2715, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 7 Loss: 0.27154794335365295\n",
      "tensor(0.2952, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 7 Loss: 0.29516473412513733\n",
      "tensor(0.3845, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 7 Loss: 0.3844957947731018\n",
      "tensor(0.3809, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 7 Loss: 0.3809289336204529\n",
      "\n",
      " Epoch 7/50\n",
      "tensor(0.2718, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 8 Loss: 0.2717604637145996\n",
      "tensor(0.2770, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 8 Loss: 0.27697238326072693\n",
      "tensor(0.2910, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 8 Loss: 0.2909512221813202\n",
      "tensor(0.2754, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 8 Loss: 0.2754039764404297\n",
      "tensor(0.2439, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 8 Loss: 0.2438930869102478\n",
      "\n",
      " Epoch 8/50\n",
      "tensor(0.2099, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 9 Loss: 0.2098860740661621\n",
      "tensor(0.1966, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 9 Loss: 0.19664259254932404\n",
      "tensor(0.1671, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 9 Loss: 0.16707876324653625\n",
      "tensor(0.1373, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 9 Loss: 0.13731077313423157\n",
      "tensor(0.2484, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 9 Loss: 0.24835847318172455\n",
      "\n",
      " Epoch 9/50\n",
      "tensor(0.1216, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 10 Loss: 0.12158074975013733\n",
      "tensor(0.1522, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 10 Loss: 0.15223807096481323\n",
      "tensor(0.1022, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 10 Loss: 0.10221771895885468\n",
      "tensor(0.1350, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 10 Loss: 0.13502022624015808\n",
      "tensor(0.1378, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 10 Loss: 0.13783057034015656\n",
      "\n",
      " Epoch 10/50\n",
      "tensor(0.1051, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 11 Loss: 0.10506924986839294\n",
      "tensor(0.0377, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 11 Loss: 0.037743210792541504\n",
      "tensor(0.1059, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 11 Loss: 0.10585945844650269\n",
      "tensor(0.0527, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 11 Loss: 0.052684154361486435\n",
      "tensor(0.0705, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 11 Loss: 0.07051754742860794\n",
      "Model saved at epoch 10\n",
      "\n",
      " Epoch 11/50\n",
      "tensor(0.0644, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 12 Loss: 0.06441405415534973\n",
      "tensor(0.0362, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 12 Loss: 0.03622470051050186\n",
      "tensor(0.3912, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 12 Loss: 0.391232967376709\n",
      "tensor(0.0110, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 12 Loss: 0.011016065254807472\n",
      "tensor(0.1271, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 12 Loss: 0.1271396279335022\n",
      "\n",
      " Epoch 12/50\n",
      "tensor(0.0682, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 13 Loss: 0.0682198777794838\n",
      "tensor(0.4112, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 13 Loss: 0.41115254163742065\n",
      "tensor(0.1522, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 13 Loss: 0.15216770768165588\n",
      "tensor(0.1365, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 13 Loss: 0.13652688264846802\n",
      "tensor(0.0648, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 13 Loss: 0.06480609625577927\n",
      "\n",
      " Epoch 13/50\n",
      "tensor(0.0857, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 14 Loss: 0.08574925363063812\n",
      "tensor(0.0703, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 14 Loss: 0.07032094895839691\n",
      "tensor(0.1236, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 14 Loss: 0.12362967431545258\n",
      "tensor(0.0556, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 14 Loss: 0.055624593049287796\n",
      "tensor(0.0857, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 14 Loss: 0.08574368059635162\n",
      "\n",
      " Epoch 14/50\n",
      "tensor(0.1105, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 15 Loss: 0.11051446199417114\n",
      "tensor(0.0762, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 15 Loss: 0.07617685943841934\n",
      "tensor(0.0908, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 15 Loss: 0.09075617790222168\n",
      "tensor(0.0968, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 15 Loss: 0.09677467495203018\n",
      "tensor(0.0955, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 15 Loss: 0.09552422910928726\n",
      "\n",
      " Epoch 15/50\n",
      "tensor(0.1134, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 16 Loss: 0.11339900642633438\n",
      "tensor(0.0388, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 16 Loss: 0.03881296142935753\n",
      "tensor(0.0401, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 16 Loss: 0.040118344128131866\n",
      "tensor(0.0610, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 16 Loss: 0.06100441887974739\n",
      "tensor(0.0299, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 16 Loss: 0.029871009290218353\n",
      "\n",
      " Epoch 16/50\n",
      "tensor(0.0471, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 17 Loss: 0.047141946852207184\n",
      "tensor(0.0370, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 17 Loss: 0.03704170137643814\n",
      "tensor(0.0346, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 17 Loss: 0.03459157794713974\n",
      "tensor(0.0744, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 17 Loss: 0.07435200363397598\n",
      "tensor(0.0164, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 17 Loss: 0.016429638490080833\n",
      "\n",
      " Epoch 17/50\n",
      "tensor(0.0459, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 18 Loss: 0.045891083776950836\n",
      "tensor(0.0640, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 18 Loss: 0.06396351009607315\n",
      "tensor(0.0105, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 18 Loss: 0.01047844160348177\n",
      "tensor(0.0194, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 18 Loss: 0.019365347921848297\n",
      "tensor(0.0110, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 18 Loss: 0.011037096381187439\n",
      "\n",
      " Epoch 18/50\n",
      "tensor(0.0176, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 19 Loss: 0.017558027058839798\n",
      "tensor(0.0211, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 19 Loss: 0.021114006638526917\n",
      "tensor(0.0211, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 19 Loss: 0.02107331156730652\n",
      "tensor(0.0102, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 19 Loss: 0.010194231756031513\n",
      "tensor(0.0113, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 19 Loss: 0.011343627236783504\n",
      "\n",
      " Epoch 19/50\n",
      "tensor(0.0064, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 20 Loss: 0.006415678188204765\n",
      "tensor(0.0217, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 20 Loss: 0.021689098328351974\n",
      "tensor(0.0300, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 20 Loss: 0.029982056468725204\n",
      "tensor(0.0054, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 20 Loss: 0.005449353251606226\n",
      "tensor(0.0066, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 20 Loss: 0.006625516805797815\n",
      "\n",
      " Epoch 20/50\n",
      "tensor(0.0182, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 21 Loss: 0.01823890209197998\n",
      "tensor(0.0014, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 21 Loss: 0.0014316444285213947\n",
      "tensor(0.0076, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 21 Loss: 0.007587113883346319\n",
      "tensor(0.0072, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 21 Loss: 0.007232220843434334\n",
      "tensor(0.0129, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 21 Loss: 0.0129090566188097\n",
      "Model saved at epoch 20\n",
      "\n",
      " Epoch 21/50\n",
      "tensor(0.0075, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 22 Loss: 0.007484729867428541\n",
      "tensor(0.0088, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 22 Loss: 0.008841666392982006\n",
      "tensor(0.0049, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 22 Loss: 0.004917178303003311\n",
      "tensor(0.0145, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 22 Loss: 0.01446764636784792\n",
      "tensor(0.0041, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 22 Loss: 0.004143633879721165\n",
      "\n",
      " Epoch 22/50\n",
      "tensor(0.0121, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 23 Loss: 0.012135136872529984\n",
      "tensor(0.0030, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 23 Loss: 0.0030091109219938517\n",
      "tensor(0.0033, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 23 Loss: 0.003274169284850359\n",
      "tensor(0.0100, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 23 Loss: 0.009986216202378273\n",
      "tensor(0.0015, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 23 Loss: 0.0015004347078502178\n",
      "\n",
      " Epoch 23/50\n",
      "tensor(0.0071, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 24 Loss: 0.007116206921637058\n",
      "tensor(0.0033, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 24 Loss: 0.0033195577561855316\n",
      "tensor(0.0060, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 24 Loss: 0.006018945947289467\n",
      "tensor(0.0029, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 24 Loss: 0.0028841481544077396\n",
      "tensor(0.0030, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 24 Loss: 0.0030443540308624506\n",
      "\n",
      " Epoch 24/50\n",
      "tensor(0.0039, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 25 Loss: 0.003866213606670499\n",
      "tensor(0.0090, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 25 Loss: 0.008962897583842278\n",
      "tensor(0.0023, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 25 Loss: 0.0022887415252625942\n",
      "tensor(0.0011, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 25 Loss: 0.001086641103029251\n",
      "tensor(0.0028, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 25 Loss: 0.0028408688958734274\n",
      "\n",
      " Epoch 25/50\n",
      "tensor(0.0017, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 26 Loss: 0.0016961375949904323\n",
      "tensor(0.0077, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 26 Loss: 0.007720420137047768\n",
      "tensor(0.0028, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 26 Loss: 0.00280994875356555\n",
      "tensor(0.0021, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 26 Loss: 0.0020793979056179523\n",
      "tensor(0.0011, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 26 Loss: 0.0010526436381042004\n",
      "\n",
      " Epoch 26/50\n",
      "tensor(0.0037, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 27 Loss: 0.0037394941318780184\n",
      "tensor(0.0018, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 27 Loss: 0.0018399306572973728\n",
      "tensor(0.0023, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 27 Loss: 0.002283746376633644\n",
      "tensor(0.0046, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 27 Loss: 0.004558461718261242\n",
      "tensor(0.0008, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 27 Loss: 0.0007649707840755582\n",
      "\n",
      " Epoch 27/50\n",
      "tensor(0.0025, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 28 Loss: 0.002542604459449649\n",
      "tensor(0.0020, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 28 Loss: 0.002025286201387644\n",
      "tensor(0.0034, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 28 Loss: 0.0033605294302105904\n",
      "tensor(0.0029, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 28 Loss: 0.0029291429091244936\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 28 Loss: 0.0009555119322612882\n",
      "\n",
      " Epoch 28/50\n",
      "tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 29 Loss: 0.0013199775712564588\n",
      "tensor(0.0041, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 29 Loss: 0.004061412997543812\n",
      "tensor(0.0024, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 29 Loss: 0.0024047631304711103\n",
      "tensor(0.0020, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 29 Loss: 0.0020020008087158203\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 29 Loss: 0.00047008981346152723\n",
      "\n",
      " Epoch 29/50\n",
      "tensor(0.0023, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 30 Loss: 0.0023421018850058317\n",
      "tensor(0.0022, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 30 Loss: 0.0021917091216892004\n",
      "tensor(0.0018, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 30 Loss: 0.001839806092903018\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 30 Loss: 0.0006459535798057914\n",
      "tensor(0.0021, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 30 Loss: 0.0021317051723599434\n",
      "\n",
      " Epoch 30/50\n",
      "tensor(0.0022, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 31 Loss: 0.0021758130751550198\n",
      "tensor(0.0012, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 31 Loss: 0.0011718675959855318\n",
      "tensor(0.0015, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 31 Loss: 0.0015005103778094053\n",
      "tensor(0.0033, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 31 Loss: 0.0032542531844228506\n",
      "tensor(0.0003, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 31 Loss: 0.000283181230770424\n",
      "Model saved at epoch 30\n",
      "\n",
      " Epoch 31/50\n",
      "tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 32 Loss: 0.001298784976825118\n",
      "tensor(0.0017, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 32 Loss: 0.001661614514887333\n",
      "tensor(0.0031, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 32 Loss: 0.0030809531453996897\n",
      "tensor(0.0008, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 32 Loss: 0.0007757684797979891\n",
      "tensor(0.0008, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 32 Loss: 0.0007622189586982131\n",
      "\n",
      " Epoch 32/50\n",
      "tensor(0.0012, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 33 Loss: 0.0012120086466893554\n",
      "tensor(0.0011, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 33 Loss: 0.0011081951670348644\n",
      "tensor(0.0027, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 33 Loss: 0.0026812036521732807\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 33 Loss: 0.0003909527731593698\n",
      "tensor(0.0016, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 33 Loss: 0.0015656298492103815\n",
      "\n",
      " Epoch 33/50\n",
      "tensor(0.0011, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 34 Loss: 0.0011052493937313557\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 34 Loss: 0.0008967330795712769\n",
      "tensor(0.0014, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 34 Loss: 0.0013821228640154004\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 34 Loss: 0.0010240876581519842\n",
      "tensor(0.0021, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 34 Loss: 0.002143333200365305\n",
      "\n",
      " Epoch 34/50\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 35 Loss: 0.0010296674445271492\n",
      "tensor(0.0015, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 35 Loss: 0.0015271921874955297\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 35 Loss: 0.0008713366696611047\n",
      "tensor(0.0021, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 35 Loss: 0.002130206674337387\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 35 Loss: 0.0003853690577670932\n",
      "\n",
      " Epoch 35/50\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 36 Loss: 0.0005037357332184911\n",
      "tensor(0.0029, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 36 Loss: 0.0029143239371478558\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 36 Loss: 0.00043395854299888015\n",
      "tensor(0.0011, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 36 Loss: 0.0010919885244220495\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 36 Loss: 0.0006178158218972385\n",
      "\n",
      " Epoch 36/50\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 37 Loss: 0.0008925572037696838\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 37 Loss: 0.0009587029344402254\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 37 Loss: 0.0004033453587908298\n",
      "tensor(0.0018, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 37 Loss: 0.0017820695647969842\n",
      "tensor(0.0012, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 37 Loss: 0.0011582843726500869\n",
      "\n",
      " Epoch 37/50\n",
      "tensor(0.0021, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 38 Loss: 0.002084496198222041\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 38 Loss: 0.0004152664332650602\n",
      "tensor(0.0008, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 38 Loss: 0.0007500539068132639\n",
      "tensor(0.0003, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 38 Loss: 0.0002692513517104089\n",
      "tensor(0.0014, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 38 Loss: 0.0014103826833888888\n",
      "\n",
      " Epoch 38/50\n",
      "tensor(0.0003, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 39 Loss: 0.00030137793510220945\n",
      "tensor(0.0017, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 39 Loss: 0.001745146932080388\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 39 Loss: 0.0005884108832105994\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 39 Loss: 0.00048361907829530537\n",
      "tensor(0.0014, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 39 Loss: 0.0014464660780504346\n",
      "\n",
      " Epoch 39/50\n",
      "tensor(0.0002, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 40 Loss: 0.00023871877056080848\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 40 Loss: 0.0004906313843093812\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 40 Loss: 0.0009513830882497132\n",
      "tensor(0.0022, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 40 Loss: 0.0022398880682885647\n",
      "tensor(0.0003, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 40 Loss: 0.0003496087738312781\n",
      "\n",
      " Epoch 40/50\n",
      "tensor(0.0007, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 41 Loss: 0.0007042284123599529\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 41 Loss: 0.0003954495768994093\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 41 Loss: 0.0006372663192451\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 41 Loss: 0.0010041496716439724\n",
      "tensor(0.0014, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 41 Loss: 0.0013667299645021558\n",
      "Model saved at epoch 40\n",
      "\n",
      " Epoch 41/50\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 42 Loss: 0.0005449668387882411\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 42 Loss: 0.0004717919509857893\n",
      "tensor(0.0012, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 42 Loss: 0.0011615807889029384\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 42 Loss: 0.000585123139899224\n",
      "tensor(0.0011, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 42 Loss: 0.0010890478733927011\n",
      "\n",
      " Epoch 42/50\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 43 Loss: 0.0006036293925717473\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 43 Loss: 0.00050552241737023\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 43 Loss: 0.0006497765425592661\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 43 Loss: 0.000974737573415041\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 43 Loss: 0.0009371500345878303\n",
      "\n",
      " Epoch 43/50\n",
      "tensor(0.0002, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 44 Loss: 0.00024125431082211435\n",
      "tensor(0.0011, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 44 Loss: 0.0010967872804030776\n",
      "tensor(0.0008, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 44 Loss: 0.0007592992624267936\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 44 Loss: 0.00047960347728803754\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 44 Loss: 0.0008654355769976974\n",
      "\n",
      " Epoch 44/50\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 45 Loss: 0.0005463476409204304\n",
      "tensor(0.0007, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 45 Loss: 0.0007133585168048739\n",
      "tensor(0.0008, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 45 Loss: 0.0008101126877591014\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 45 Loss: 0.000910319562535733\n",
      "tensor(0.0003, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 45 Loss: 0.0002727291139308363\n",
      "\n",
      " Epoch 45/50\n",
      "tensor(0.0007, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 46 Loss: 0.0007321385201066732\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 46 Loss: 0.0005817491328343749\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 46 Loss: 0.0006205610116012394\n",
      "tensor(0.0008, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 46 Loss: 0.0008179783471859992\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 46 Loss: 0.000388684420613572\n",
      "\n",
      " Epoch 46/50\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 47 Loss: 0.0009896857663989067\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 47 Loss: 0.0004810701066162437\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 47 Loss: 0.0005109842750243843\n",
      "tensor(0.0003, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 47 Loss: 0.00034065640647895634\n",
      "tensor(0.0007, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 47 Loss: 0.0007270783535204828\n",
      "\n",
      " Epoch 47/50\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 48 Loss: 0.000924767809920013\n",
      "tensor(0.0002, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 48 Loss: 0.00018543309124652296\n",
      "tensor(0.0007, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 48 Loss: 0.0007010270492173731\n",
      "tensor(0.0002, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 48 Loss: 0.0002374702162342146\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 48 Loss: 0.0008501326665282249\n",
      "\n",
      " Epoch 48/50\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 49 Loss: 0.00041129038436338305\n",
      "tensor(0.0002, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 49 Loss: 0.00024927101912908256\n",
      "tensor(0.0009, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 49 Loss: 0.0009453397942706943\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 49 Loss: 0.0004932486917823553\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 49 Loss: 0.0005825607222504914\n",
      "\n",
      " Epoch 49/50\n",
      "tensor(0.0002, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 50 Loss: 0.0002352226001676172\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 50 Loss: 0.00040415197145193815\n",
      "tensor(0.0003, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 50 Loss: 0.0002669446403160691\n",
      "tensor(0.0005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 50 Loss: 0.0004827125230804086\n",
      "tensor(0.0012, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 50 Loss: 0.0012087839422747493\n",
      "\n",
      " Epoch 50/50\n",
      "tensor(0.0002, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 51 Loss: 0.0001979513472178951\n",
      "tensor(0.0004, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 51 Loss: 0.0003747790469788015\n",
      "tensor(0.0001, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 51 Loss: 0.00013778744323644787\n",
      "tensor(0.0007, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 51 Loss: 0.0007188307354226708\n",
      "tensor(0.0010, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 51 Loss: 0.0010139322839677334\n",
      "Model saved at epoch 50\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "train(train_loader,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c33b3a",
   "metadata": {},
   "source": [
    "Evaluating model and making visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2f6c6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6790e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.8267e-01],\n",
       "        [9.9392e-01],\n",
       "        [9.5789e-01],\n",
       "        [9.3023e-01],\n",
       "        [9.9815e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.1119e-06],\n",
       "        [9.1153e-01],\n",
       "        [9.9770e-01],\n",
       "        [9.9975e-01],\n",
       "        [9.9829e-01],\n",
       "        [9.9974e-01],\n",
       "        [7.6667e-06]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input, test_val, test_label = batch_1\n",
    "y_pred = model(test_input,test_val)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "064d9967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.tensor([1 if prediction > 0.5 else 0 for prediction in y_pred])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa2547e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9151f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7692307829856873\n",
      "accuracy: 0.8125\n",
      "recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#calculating metrics\n",
    "p = Precision(task=\"binary\")\n",
    "a = Accuracy(task=\"binary\")\n",
    "r = Recall(task=\"binary\")\n",
    "precision = p(y_pred,test_label)\n",
    "accuracy = a(y_pred,test_label)\n",
    "recall = r(y_pred,test_label)\n",
    "print(\"precision:\",precision.item())\n",
    "print(\"accuracy:\",accuracy.item())\n",
    "print(\"recall:\",recall.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c20b7",
   "metadata": {},
   "source": [
    "Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[5].permute(1,2,0))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[5].permute(1,2,0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a77128",
   "metadata": {},
   "source": [
    "Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "564083f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'siamese_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9449cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNeuralNetwork(\n",
       "  (embedding): Embedding(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(10, 10), stride=(1, 1))\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "  )\n",
       "  (L1Dist): L1Dist()\n",
       "  (fc1): Linear(in_features=4096, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reloading the model \n",
    "model = torch.load('siamese_model.pth',weights_only=False)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b28b4340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6790e-01],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.8267e-01],\n",
       "        [9.9392e-01],\n",
       "        [9.5789e-01],\n",
       "        [9.3023e-01],\n",
       "        [9.9815e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.1119e-06],\n",
       "        [9.1153e-01],\n",
       "        [9.9770e-01],\n",
       "        [9.9975e-01],\n",
       "        [9.9829e-01],\n",
       "        [9.9974e-01],\n",
       "        [7.6667e-06]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting \n",
    "prediction = model(test_input,test_val)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ac9f4",
   "metadata": {},
   "source": [
    "Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d499fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model,detection_threshold,verification_threshold):\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data','verification_images')):\n",
    "        if(image.endswith('.jpg')==False):\n",
    "            continue\n",
    "        input_img = preprocess(os.path.join('application_data','input_image','input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join('application_data','verification_images',image))\n",
    "        \n",
    "        input_img = input_img.unsqueeze(0)\n",
    "        validation_img = validation_img.unsqueeze(0)\n",
    "        \n",
    "        output = model(input_img,validation_img)\n",
    "        results.append(output.item())\n",
    "\n",
    "        #detection threshold: tells us is above which value we should consider it to be a positive match\n",
    "        #verification threshold: tells us what percentage of verification images should match to consider it verified\n",
    "        detection = np.sum(np.array(results) > detection_threshold)\n",
    "        verification = detection / len(os.listdir(os.path.join('application_data','verification_images')))\n",
    "        verified = verification > verification_threshold\n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "538e949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified: True, results: [0.9998984336853027, 0.9997974038124084, 0.07506275177001953, 0.9999966621398926, 0.999798595905304, 0.9998936653137207, 0.9999730587005615, 0.998054027557373, 0.9999892711639404, 0.9999885559082031, 0.9997119307518005, 0.9999001026153564, 0.9998830556869507, 0.14195623993873596, 0.9941732287406921, 0.07823236286640167, 0.993735134601593, 0.999997615814209, 0.9793763160705566, 0.9988034963607788, 0.9994121789932251, 0.9999897480010986, 0.9998098015785217, 0.5837090611457825, 0.7138606309890747, 0.998222291469574, 0.999901533126831, 0.9998866319656372, 0.7465131282806396, 0.9999946355819702, 0.9999963045120239, 0.9994511008262634, 0.995814859867096, 0.9998763799667358, 0.9999855756759644, 0.999996542930603, 0.8863745331764221, 0.13079853355884552, 0.9999043941497803, 0.9958627223968506, 0.22529682517051697, 0.9981738328933716, 0.06748965382575989, 0.995241641998291, 0.9999936819076538, 0.9997188448905945, 0.9766618013381958, 0.9982652068138123, 0.9999970197677612, 0.9997310042381287]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    # cutting frame to 250 by 250px\n",
    "    frame = frame[275:275+250, 550:550+250, :]\n",
    "\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    #saving input image\n",
    "    if cv2.waitKey(1) & 0XFF == ord('v'):\n",
    "        input_img_path = os.path.join('application_data','input_image','input_image.jpg')\n",
    "        cv2.imwrite(input_img_path, frame)\n",
    "        results, verified = verify(model,0.5,0.5)\n",
    "        print(f\"verified: {verified}, results: {results}\")\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66320c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SiameseNeuralNet (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
